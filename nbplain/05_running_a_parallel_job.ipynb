{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Jobs\n",
    "\n",
    "## Overview:\n",
    "\n",
    "- **Teaching:** 10 min\n",
    "- **Exercises:** 10 min\n",
    "\n",
    "**Questions**\n",
    "- How do I run a job on multiple compute nodes?\n",
    "- How do I check the restrictions on the partitions?\n",
    "\n",
    "**Objectives**\n",
    "- Compile and run a job on several nodes.\n",
    "- Check what limits are on different partitions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a parallel job\n",
    "\n",
    "So far we have run jobs on single nodes. but a we have already seen, the power of HPC systems comes from parallelism, i.e. having lots of processors/disks etc. connected together rather than having more powerful components than your laptop or workstation. Often, when running research programs on HPC you will need to run a program that has been built to use the MPI (Message Passing Interface) parallel library. The MPI library allows programs to exploit multiple processing cores in parallel to allow researchers to model or simulate faster on larger problem sizes. The details of how MPI work are not important for this course or even to use programs that have been built using MPI; however, MPI programs typically have to be launched in job submission scripts in a different way to serial programs and users of parallel programs on HPC systems need to know how to do this. Specifically, launching parallel MPI programs typically requires four things:\n",
    "\n",
    "- A special parallel launch program such as mpirun, mpiexec, srun or aprun.\n",
    "- A specification of how many processes to use in parallel. For example, our parallel program may use 256 processes in parallel.\n",
    "- A specification of how many parallel processes to use per compute node. For example, if our compute nodes each have 32 cores we often want to specify 32 parallel processes per node.\n",
    "- The command and arguments for our parallel program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello world\n",
    "\n",
    "Below is a simple parallel implementation of the hello world program written in fortran.  \n",
    "\n",
    "```fortran\n",
    "PROGRAM hello_world_mpi\n",
    "include 'mpif.h'\n",
    "\n",
    "integer process_Rank, size_Of_Cluster, ierror, tag ,status,resultlen\n",
    "\n",
    "character*(MPI_MAX_PROCESSOR_NAME) name\n",
    "\n",
    "\n",
    "call MPI_INIT(ierror)\n",
    "call MPI_COMM_SIZE(MPI_COMM_WORLD, size_Of_Cluster, ierror)\n",
    "call MPI_COMM_RANK(MPI_COMM_WORLD, process_Rank, ierror)\n",
    "call MPI_GET_PROCESSOR_NAME(name, resultlen, ierror)\n",
    "\n",
    "print *, 'Hello World from process: ',TRIM(name), process_Rank, 'of ', size_Of_Cluster\n",
    "\n",
    "call MPI_FINALIZE(ierror)\n",
    "END PROGRAM\n",
    "```\n",
    "\n",
    "Create a new file named `hello_world_mpi.f90` with the above in it. In order to run the program we must first compile it with the following command (make sure you have the openmpi module loaded first):\n",
    "\n",
    "```bash\n",
    "mpif90 hello_world_mpi.f90 -o hello_world_mpi\n",
    "```\n",
    "\n",
    "When you have issued this command you should have an executable in your directory called `hello_world_mpi`. We will now run it on our cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running in Parallel\n",
    "\n",
    "In order to run the `hello_world_mpi` program in parallel we will need to write a run script. \n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --account=prj0_phase1\n",
    "#SBATCH --job-name=myjob\n",
    "#SBATCH --partition=shortJob\n",
    "#SBATCH --qos=shortJob\n",
    "#SBATCH --time=0-00:15:00\n",
    "#SBATCH --output=%x.%j.o\n",
    "#SBATCH --error=%x.%j.e\n",
    "#SBATCH --ntasks=4\n",
    "#SBATCH --ntasks-per-node=1\n",
    "\n",
    "module load mpi/openmpi-x86_64\n",
    "mpirun -np 4 hello_world_mpi\n",
    "```\n",
    "\n",
    "We can see there have been some new additions:\n",
    "\n",
    "- `--ntasks=`: this tells slurm the **total number of processes** we would like for the job.\n",
    "- `--ntasks-per-node=`: this tells slurm the number of processes to place on each node.\n",
    "- `module load mpi/openmpi-x86_64`: in order to run the job using MPI we need to have the MPI module loaded.\n",
    "- `mpirun -np 4 hello_world_mpi`: this actually runs the program. mpirun is the program that executes the program in parallel. `-np 4` tells mpirun we want 4 processes, and `hello_world_mpi` is the name of our program. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: submit the parallel job\n",
    "\n",
    "Compile the parallel job using the instructions above and submit it with the above run script. What happens? Can you figure out why? (Hint: `scontrol` can also be used to give you information about partitions)\n",
    "\n",
    "[Solution]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: submit the parallel job\n",
    "\n",
    "The `sortJob` partition hs a maximum node limit of 2 nodes. We can see this with the following command:\n",
    "\n",
    "```bash\n",
    "scontrol show partition shortJob\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: submit the parallel job to an appropriate partition\n",
    "\n",
    "Using the `scontrol` command check the node limit on the `bigJob` partition and submit a job with the maximum number of nodes.\n",
    "\n",
    "Check the output to see where each MPI process is running. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: configuring parallel jobs\n",
    "\n",
    "On our test cluster we have 4 nodes, each with 1 CPU. Typically on HPC systems each node will have multiple CPUS. \n",
    "\n",
    "What slurm directives and mpirun argument would you use to run a job using all processes on three nodes on a partition with 16 CPU nodes?\n",
    "\n",
    "[Solution]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: configuring parallel jobs\n",
    "\n",
    "```bash\n",
    "#SBATCH --ntasks=48\n",
    "#SBATCH --ntasks-per-node=16\n",
    "\n",
    "mpirun -np 48 hello_world_mpi\n",
    "```\n",
    "\n",
    "We have 16 processes on each of three nodes, giving us 48 processes in total. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to have fine grained control over where your MPI processes are placed (see `man sbatch`, or https://slurm.schedmd.com/sbatch.html) for more information. \n",
    "\n",
    "It is also possible to simply ask for a certain number of nodes:\n",
    "\n",
    "```bash\n",
    "#SBATCH --nodes=\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points\n",
    "\n",
    "- Using the `--ntasks` and `--ntasks-per-node` directives you can tell slurm the compute resources you wish to use. \n",
    "- You can also simply specify the number of nodes with `#SBATCH --nodes=`\n",
    "- For more information on placing processes and finer control see `man sbatch`\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
