{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Scheduler\n",
    "\n",
    "## Overview:\n",
    "\n",
    "- **Teaching:** 30 min\n",
    "- **Exercises:** 30 min\n",
    "\n",
    "**Questions**\n",
    "- What is a scheduler and why are they used?\n",
    "- How do I launch a program to run on a compute node?\n",
    "- How do I check limits on the cluster and my accounts?\n",
    "\n",
    "**Objectives**\n",
    "- Run a simple job on the cluster's compute nodes.\n",
    "- Inspect the status of your job.\n",
    "- Inspect the output and error files of your job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Scheduling\n",
    "\n",
    "An HPC system might have thousands of nodes and thousands of users. How do we decide who gets what and when? How do we ensure that a task is run with the resources it needs? This job is handled by a special piece of software called the scheduler. On an HPC system, the scheduler manages which jobs run where and when.\n",
    "\n",
    "The following illustration compares these tasks of a job scheduler to a waiter in a restaurant. If you can relate to an instance where you had to wait for a while in a queue to get in to a popular restaurant, then you may now understand why sometimes your jobs do not start instantly as in your laptop.\n",
    "\n",
    "![Scheduler](../images/03/restaurant_queue_manager.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slurm: Simple Linux Utility for Resource Managment\n",
    "\n",
    "Interacting with the scheduler is done through the terminal using an array of commands. If multiple jobs ran on a single node at the same time users would be competing for the same resources and jobs take longer to run overall. A scheduler manages individual jobs, which are allocated to the resources they need as they become available. This results in a higher overall throughput and more consistent performance.\n",
    "\n",
    "The scheduler on our cluster container (and Nimbus) is `slurm`: _**S**_imple _**L**_inux _**U**_tility for _**R**_esource _**M**_anagment.\n",
    "\n",
    "Interacting with the scheduler is done through the terminal using an array of commands. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Run a bash job\n",
    "\n",
    "Using the text editor in the terminal create a bash script called `test_job.sh` with the following:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "echo 'This script is running on:'\n",
    "hostname\n",
    "sleep 60\n",
    "```\n",
    "\n",
    "And run it.\n",
    "\n",
    "Where did the script run? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: sbatch\n",
    "\n",
    "If you completed the previous challenge successfully, you probably realise that there is a distinction between running the job through the scheduler and just “running it”. To submit this job to the scheduler, we use a command to submit the job to slurm:\n",
    "\n",
    "```bash\n",
    "jupyter-user:$ sbatch test_job.sh\n",
    "```\n",
    "\n",
    "Try this now. What happens?\n",
    "\n",
    "[Solution]()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: sbatch\n",
    "We get the following error:\n",
    "\n",
    "```bash\n",
    "sbatch: error: Batch job submission failed: Invalid qos specification\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When submitting a job you typically have to give the scheduler, slurm, information in order to help it decide where and when to run the job. \n",
    "\n",
    "If there are lots of different compute nodes, for example, we might want to make sure our job goes on a particular CPU. And we will have to tell `slurm` how long our job will run for so it can make decisions about where to put jobs. \n",
    "\n",
    "So lets have a look at a more complete job script:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --account=prj0_phase1\n",
    "#SBATCH --job-name=myjob\n",
    "#SBATCH --partition=shortJob\n",
    "#SBATCH --qos=shortJob\n",
    "#SBATCH --time=0-00:25:00\n",
    "\n",
    "echo 'This script is running on:'\n",
    "hostname\n",
    "sleep 60\n",
    "```\n",
    "\n",
    "Every line that starts with a `#SBATCH` is a directive for `slurm` this is where we tell slurm the resources we want for our job. \n",
    "\n",
    "`#SBATCH --account=` tells slurm what account you wish to run against. Slurm will select a default account, f there is one, but it is better practice to make sure you include this. (In order to run a job on Nimbus you need to have a resource allocation, with funds, and it is this code that you put here)\n",
    "\n",
    "`#SBATCH --job-name=` gives the job a name, which you can identify in the queue - you can call the job whatever you want that helps you to identify it. \n",
    "\n",
    "`#SBATCH --partition=` tells slurm what partition to put the job on. Typically a HPC system will have different partitions which will have different resources or limits on them. For example, there may be a partion with a particular CPU type, or one with limits on the size or duration of job you can run. \n",
    "\n",
    "`#SBATCH --qos=` tells slurm what Quality of Service you wish to run with. The QOS is simply a way for admins to apply rules to the resources you can access, and the priority of the job. For Bath's HPC systems the qos will match the partition name - and **remember** if you don't include it you will get an error.\n",
    "\n",
    "`#SBATCH --time=` tells slurm how long you wish to run the job for, with several acceptable formats:  \"minutes\", \"minutes:seconds\", \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\" and \"days-hours:minutes:seconds\" e.g. `1-01:20:00` will request a runtime of 25 hours and 20 mins. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Running a batch job\n",
    "\n",
    "Try editing your runscript to match the example above and submit it. \n",
    "\n",
    "What happens?\n",
    "\n",
    "[Solution]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Running a batch job\n",
    "\n",
    "Again we get an error. \n",
    "\n",
    "```sbatch: error: Batch job submission failed: Requested time limit is invalid (missing or exceeds some limit)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sinfo\n",
    "\n",
    "We get an error above because the time limit on the partition has been exceeded. We can use a `slurm` command `sinfo` to get information about what partitions are available:\n",
    "\n",
    "```bash\n",
    "jupyter-user:$sinfo\n",
    "PARTITION      AVAIL  TIMELIMIT  NODES  STATE NODELIST\n",
    "PARTITION     AVAIL  TIMELIMIT  NODES  STATE NODELIST\n",
    "bigJob           up 1-00:00:00      4   idle c[1-4]\n",
    "shortJob         up      15:00      4   idle c[1-4]\n",
    "notallowedjob    up      15:00      4   idle c[1-4]\n",
    "```\n",
    "\n",
    "We can see there are three different partitions. `bigJob`, `shortJob` and `notallowedjob`. All are available (we can see up in `AVAIL` column), next we can see the time limit on jobs allowed to run un that partition. \n",
    "\n",
    "Now we can see why our job was rejected - we requested a 25 minute runtime on the `shortjob` partition, when the timelimit is only 15 minutes. \n",
    "Try reducing the time requested in the runscript and resubmitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sacctmgr\n",
    "\n",
    "Yet another error:\n",
    "\n",
    "```bash\n",
    "sbatch: error: Batch job submission failed: Invalid account or account/partition combination specified\n",
    "```\n",
    "\n",
    "So we need to check what accounts and QOS we have access to. We can do that with the `sacctmgr` command:\n",
    "\n",
    "```\n",
    "sacctmgr show associations user=jupyter-user --parsable2 format=account,user\n",
    "\n",
    "```\n",
    "\n",
    "This shows the associations between the QOS and our user account, output only what we are interested in (`format=account,user`) in a readbale format (`--parsable2`). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: adding the account\n",
    "\n",
    "Using the sacctmgr command take note of the account you have access to and update your submission script, and resunbmit it using the sbatch command. \n",
    "\n",
    "Where was the script executed? Have any new files been created?\n",
    "\n",
    "[Solution]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: adding the account\n",
    "\n",
    "If you have successfully updated your runscript it should now be able to be submitted, and should run on one of the compute nodes (c*). \n",
    "\n",
    "A file named `slurm-<jobid>.out` will have been created. This is the stdout from your run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redirecting output\n",
    "\n",
    "We have seen that a new file was created by our successful submission `slurm-<jobid>.out`. We can tailor where our output goes with a couple of slurm directives:\n",
    "\n",
    "`#SBATCH --output=` will tell where to put the output\n",
    "`#SBATCH --error=` will tell slurm where to send any error messages\n",
    "\n",
    "We can also incorporate the job name with `%x` and the job id with `%j` to help identify our output files and ensure they arent overwritten:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Redirecting output\n",
    "\n",
    "Include the following in your run script and resubmit.\n",
    "\n",
    "```bash\n",
    "#SBATCH --output=%x.%j.o\n",
    "#SBATCH --error=%x.%j.e\n",
    "```\n",
    "\n",
    "What happens to your output now?\n",
    "\n",
    "[Solution]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Redirecting output\n",
    "\n",
    "Your output should have been sent to a file called `myjob.<jobid>.o` where `<jobid>` is a numeric value identifying your job. If there were any errors written they would have been put in a file called `myjob.<jobid>.e`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring your jobs\n",
    "\n",
    "Obviously at some points your jobs may have to wait to run, if all the resources are currently being used. Jobs waiting to be run will be placed in a queue. There are several commands we can use to check on our jobs. \n",
    "\n",
    "The command `squeue` will show us the current queue, \n",
    "\n",
    "```bash\n",
    "jupyter-user:$squeue\n",
    "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
    "                 7  shortJob    myjob jupyter- PD       0:00      1 (Resources)\n",
    "                 8  shortJob    myjob jupyter- PD       0:00      1 (Priority)\n",
    "                 9  shortJob    myjob jupyter- PD       0:00      1 (Priority)\n",
    "                10  shortJob    myjob jupyter- PD       0:00      1 (Priority)\n",
    "                11  shortJob    myjob jupyter- PD       0:00      1 (Priority)\n",
    "                12  shortJob    myjob jupyter- PD       0:00      1 (Priority)\n",
    "                13  shortJob    myjob jupyter- PD       0:00      1 (Priority)\n",
    "                14  shortJob    myjob jupyter- PD       0:00      1 (Priority)\n",
    "                15  shortJob    myjob jupyter- PD       0:00      1 (Priority)\n",
    "                16  shortJob    myjob jupyter- PD       0:00      1 (Priority)\n",
    "                17  shortJob    myjob jupyter- PD       0:00      1 (Priority)\n",
    "                18  shortJob    myjob jupyter- PD       0:00      1 (Priority)\n",
    "                19  shortJob    myjob jupyter- PD       0:00      1 (Priority)\n",
    "                20  shortJob    myjob jupyter- PD       0:00      1 (Priority)\n",
    "                21  shortJob    myjob jupyter- PD       0:00      1 (Priority)\n",
    "                22  shortJob    myjob jupyter- PD       0:00      1 (Priority)\n",
    "                23  shortJob    myjob jupyter- PD       0:00      1 (Priority)\n",
    "                 3  shortJob    myjob jupyter-  R       0:32      1 c1\n",
    "                 4  shortJob    myjob jupyter-  R       0:02      1 c2\n",
    "                 5  shortJob    myjob jupyter-  R       0:02      1 c3\n",
    "                 6  shortJob    myjob jupyter-  R       0:02      1 c4\n",
    "```\n",
    "\n",
    "with a numeric jobid (JOBID), the partition details (PARTITION), the job name (NAME), user that owns the job (USER), the job state (ST), runtime (TIME), the number of nodes requested (NODES), and the nodes it is running on or a reason why it is queued NODELIST(REASON).  \n",
    "\n",
    "`squeue` will accept arguments (remember you can see what arguments with the command `man squeue`) but probably the most useful one is to pass your username thorugh with `squeue -u jupyterhub-user` which will show only your jobs. \n",
    "\n",
    "There is another command that will produce more detailed information about your jobs:\n",
    "\n",
    "```bash\n",
    "jupyter-user:$scontrol show job <jobid>\n",
    "JobId=44 JobName=myjob\n",
    "   UserId=jupyter-cor22(1000) GroupId=jupyter-cor22(1000) MCS_label=N/A\n",
    "   Priority=4294901716 Nice=0 Account=prj0_phase1 QOS=shortjob\n",
    "   JobState=RUNNING Reason=None Dependency=(null)\n",
    "   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n",
    "   RunTime=00:00:03 TimeLimit=00:15:00 TimeMin=N/A\n",
    "   SubmitTime=2023-02-06T22:07:26 EligibleTime=2023-02-06T22:07:26\n",
    "   AccrueTime=2023-02-06T22:07:26\n",
    "   StartTime=2023-02-06T22:07:26 EndTime=2023-02-06T22:22:26 Deadline=N/A\n",
    "   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2023-02-06T22:07:26 Scheduler=Main\n",
    "   Partition=shortJob AllocNode:Sid=login:606\n",
    "   ReqNodeList=(null) ExcNodeList=(null)\n",
    "   NodeList=c1\n",
    "   BatchHost=c1\n",
    "   NumNodes=1 NumCPUs=1 NumTasks=1 CPUs/Task=1 ReqB:S:C:T=0:0:*:*\n",
    "   TRES=cpu=1,mem=500M,node=1,billing=1\n",
    "   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n",
    "   MinCPUsNode=1 MinMemoryCPU=500M MinTmpDiskNode=0\n",
    "   Features=(null) DelayBoot=00:00:00\n",
    "   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n",
    "   Command=/data/jupyter-cor22/test.sh\n",
    "   WorkDir=/data/jupyter-cor22\n",
    "   StdErr=/data/jupyter-cor22/myjob.44.e\n",
    "   StdIn=/dev/null\n",
    "   StdOut=/data/jupyter-cor22/myjob.44.o\n",
    "   Power=\n",
    "   ```\n",
    "\n",
    "Try submitting a job and monitoring it with `squeue` and `scontrol` now. Note the different job states in shorthand from `squeue` (`R`,`PD` etc) and long form from `scontrol` (`Running`,`Pending etc`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Cancelling a job\n",
    "\n",
    "The last command you should know is the one to cancel a job, as mistakes in job submissions can and often do happen. \n",
    "\n",
    "To cancel a job you use the command:\n",
    "\n",
    "```bash\n",
    "jupyter-user:$scancel <jobid>\n",
    "```\n",
    "\n",
    "Where `<jobid>` is the numeric job id you can get from squeue or scontrol. \n",
    "\n",
    "Try submitting and cancelling a job now. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Here is alist of the key `slurm` command you will need to interact with the scheduler. \n",
    "\n",
    "| Slurm command | Function |\n",
    "|---|---|\n",
    "| `sinfo` | View information about SLURM nodes and partitions |\n",
    "| `squeue` | List status of jobs in the **queue** |\n",
    "| `squeue --user [userid]` | Jobs by user |\n",
    "| `squeue --job [jobid]` | Jobs by jobid |\n",
    "| `sbatch [jobscript]` | Submit a jobscript to the scheduler |\n",
    "| `scancel [jobid]` | Cancel a job in the queue |\n",
    "| `scontrol hold [jobid]` | Hold a job in the queue |\n",
    "| `scontrol release [jobid]` | Release a held job |\n",
    "| `scontrol show job [jobid]` | View information about a job |\n",
    "| `scontrol show node nodename` | Get information of a node |\n",
    "| `scontrol show license` | Get licenses available on SLURM |\n",
    "\n",
    "You can find out more details in the slurm documentation here:  http://slurm.schedmd.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points:\n",
    "- We use a scheduler to manage jobs on the cloud HPC service\n",
    "- Nimbus uses the slurm scheduler\n",
    "- Key commands are:\n",
    "    * `sbatch` to submit jobs\n",
    "    * `sinfo` to view information about the service\n",
    "    * `squeue` to view the queue\n",
    "    * `scancel` to delete a job\n",
    "    * `scontrol` to show and control jobs\n",
    "    * `sacctmgr` to query account details\n",
    "\n",
    "You can find further information about slurm and the commands here: http://slurm.schedmd.com/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
